#!/usr/bin/env python3
"""
ULTIMATE AI HIVE MIND - 30 BEST MODELS
Autonomous perfection-seeking with AI consensus voting

This is the world's most comprehensive AI hive mind for algorithmic trading,
featuring 30 of the best AI models from OpenRouter working together to achieve
absolute perfection.
"""

import os
import json
import asyncio
import requests
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
from enum import Enum

class SignalType(Enum):
    """Trading signal types"""
    BUY = "BUY"
    SELL = "SELL"
    HOLD = "HOLD"
    STRONG_BUY = "STRONG_BUY"
    STRONG_SELL = "STRONG_SELL"

@dataclass
class AIModelConfig:
    """Configuration for an AI model"""
    id: str
    name: str
    provider: str
    weight: float
    specialization: str
    cost_per_1m_tokens: float

@dataclass
class AIVote:
    """AI model vote on trading decision"""
    model_id: str
    model_name: str
    signal: SignalType
    confidence: float
    reasoning: str
    timestamp: datetime
    response_time: float

class UltimateAIHiveMind:
    """
    ULTIMATE AI HIVE MIND - 30 BEST MODELS
    
    Features:
    - 30 top AI models from OpenRouter
    - Weighted consensus voting
    - Specialized model roles
    - Autonomous perfection-seeking
    - Real-time decision making
    """
    
    def __init__(self):
        """Initialize the ultimate AI hive mind"""
        self.openrouter_key = os.getenv("OPENROUTER_API_KEY", "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")
        
        # Configure 30 best models for trading
        self.models = self._configure_30_best_models()
        
        self.consensus_threshold = 0.70  # 70% agreement required
        self.min_votes_required = 20  # At least 20 models must vote
        
        print(f"ðŸ¤– ULTIMATE AI HIVE MIND initialized with {len(self.models)} models")
        print(f"   Total specializations: {len(set(m.specialization for m in self.models.values()))}")
        print(f"   Consensus threshold: {self.consensus_threshold:.0%}")
        
    def _configure_30_best_models(self) -> Dict[str, AIModelConfig]:
        """Configure the 30 best AI models for trading"""
        models = {}
        
        # Tier 1: Advanced Reasoning Models (30% weight)
        models["cogito-405b"] = AIModelConfig(
            id="deepcogito/cogito-v2-preview-llama-405b",
            name="Deep Cogito V2 405B",
            provider="Deep Cogito",
            weight=0.10,
            specialization="advanced_reasoning",
            cost_per_1m_tokens=3.50
        )
        
        models["gpt5-codex"] = AIModelConfig(
            id="openai/gpt-5-codex",
            name="GPT-5 Codex",
            provider="OpenAI",
            weight=0.08,
            specialization="code_analysis",
            cost_per_1m_tokens=10.0
        )
        
        models["claude-sonnet-4.5"] = AIModelConfig(
            id="anthropic/claude-sonnet-4.5",
            name="Claude Sonnet 4.5",
            provider="Anthropic",
            weight=0.07,
            specialization="strategic_planning",
            cost_per_1m_tokens=5.0
        )
        
        models["o3-deep-research"] = AIModelConfig(
            id="openai/o3-deep-research",
            name="o3 Deep Research",
            provider="OpenAI",
            weight=0.05,
            specialization="deep_research",
            cost_per_1m_tokens=10.0
        )
        
        # Tier 2: Fast Reasoning Models (25% weight)
        models["grok-4-fast"] = AIModelConfig(
            id="x-ai/grok-4-fast",
            name="Grok 4 Fast",
            provider="xAI",
            weight=0.06,
            specialization="fast_reasoning",
            cost_per_1m_tokens=2.0
        )
        
        models["claude-haiku-4.5"] = AIModelConfig(
            id="anthropic/claude-haiku-4.5",
            name="Claude Haiku 4.5",
            provider="Anthropic",
            weight=0.06,
            specialization="fast_execution",
            cost_per_1m_tokens=1.0
        )
        
        models["gemini-2.5-flash"] = AIModelConfig(
            id="google/gemini-2.5-flash-preview-09-2025",
            name="Gemini 2.5 Flash",
            provider="Google",
            weight=0.05,
            specialization="multimodal_analysis",
            cost_per_1m_tokens=0.5
        )
        
        models["qwen3-max"] = AIModelConfig(
            id="qwen/qwen3-max",
            name="Qwen3 Max",
            provider="Qwen",
            weight=0.05,
            specialization="maximum_capability",
            cost_per_1m_tokens=1.0
        )
        
        models["grok-code-fast"] = AIModelConfig(
            id="x-ai/grok-code-fast-1",
            name="Grok Code Fast 1",
            provider="xAI",
            weight=0.03,
            specialization="code_optimization",
            cost_per_1m_tokens=1.5
        )
        
        # Tier 3: Specialized Models (25% weight)
        models["deepseek-v3.2"] = AIModelConfig(
            id="deepseek/deepseek-v3.2-exp",
            name="DeepSeek V3.2 Exp",
            provider="DeepSeek",
            weight=0.05,
            specialization="experimental_features",
            cost_per_1m_tokens=0.5
        )
        
        models["qwen3-coder-plus"] = AIModelConfig(
            id="qwen/qwen3-coder-plus",
            name="Qwen3 Coder Plus",
            provider="Qwen",
            weight=0.04,
            specialization="advanced_coding",
            cost_per_1m_tokens=0.8
        )
        
        models["ling-1t"] = AIModelConfig(
            id="inclusionai/ling-1t",
            name="Ling-1T",
            provider="inclusionAI",
            weight=0.04,
            specialization="trillion_params",
            cost_per_1m_tokens=0.57
        )
        
        models["qwen3-vl-thinking"] = AIModelConfig(
            id="qwen/qwen3-vl-8b-thinking",
            name="Qwen3 VL Thinking",
            provider="Qwen",
            weight=0.03,
            specialization="visual_reasoning",
            cost_per_1m_tokens=0.18
        )
        
        models["nemotron-super"] = AIModelConfig(
            id="nvidia/llama-3.3-nemotron-super-49b-v1.5",
            name="Nemotron Super 49B",
            provider="NVIDIA",
            weight=0.03,
            specialization="optimized_reasoning",
            cost_per_1m_tokens=0.5
        )
        
        models["glm-4.6"] = AIModelConfig(
            id="z-ai/glm-4.6",
            name="GLM 4.6",
            provider="Z.AI",
            weight=0.03,
            specialization="general_intelligence",
            cost_per_1m_tokens=0.8
        )
        
        models["step3"] = AIModelConfig(
            id="stepfun-ai/step3",
            name="Step3",
            provider="StepFun",
            weight=0.03,
            specialization="step_by_step_reasoning",
            cost_per_1m_tokens=1.0
        )
        
        # Tier 4: Efficient Models (20% weight)
        models["qwen3-coder-flash"] = AIModelConfig(
            id="qwen/qwen3-coder-flash",
            name="Qwen3 Coder Flash",
            provider="Qwen",
            weight=0.03,
            specialization="fast_coding",
            cost_per_1m_tokens=0.3
        )
        
        models["gemini-flash-lite"] = AIModelConfig(
            id="google/gemini-2.5-flash-lite-preview-09-2025",
            name="Gemini Flash Lite",
            provider="Google",
            weight=0.03,
            specialization="efficient_analysis",
            cost_per_1m_tokens=0.2
        )
        
        models["qwen-plus"] = AIModelConfig(
            id="qwen/qwen-plus-2025-07-28",
            name="Qwen Plus",
            provider="Qwen",
            weight=0.02,
            specialization="balanced_performance",
            cost_per_1m_tokens=0.5
        )
        
        models["nemotron-nano"] = AIModelConfig(
            id="nvidia/nemotron-nano-9b-v2",
            name="Nemotron Nano 9B",
            provider="NVIDIA",
            weight=0.02,
            specialization="lightweight_reasoning",
            cost_per_1m_tokens=0.2
        )
        
        models["arcee-afm"] = AIModelConfig(
            id="arcee-ai/afm-4.5b",
            name="AFM 4.5B",
            provider="Arcee AI",
            weight=0.02,
            specialization="instruction_tuned",
            cost_per_1m_tokens=0.1
        )
        
        models["longcat-flash"] = AIModelConfig(
            id="meituan/longcat-flash-chat",
            name="LongCat Flash",
            provider="Meituan",
            weight=0.02,
            specialization="long_context",
            cost_per_1m_tokens=0.3
        )
        
        models["kimi-k2"] = AIModelConfig(
            id="moonshotai/kimi-k2-0905",
            name="Kimi K2",
            provider="MoonshotAI",
            weight=0.02,
            specialization="creative_reasoning",
            cost_per_1m_tokens=0.5
        )
        
        models["internvl3"] = AIModelConfig(
            id="opengvlab/internvl3-78b",
            name="InternVL3 78B",
            provider="OpenGVLab",
            weight=0.02,
            specialization="vision_language",
            cost_per_1m_tokens=0.6
        )
        
        models["cydonia-24b"] = AIModelConfig(
            id="thedrummer/cydonia-24b-v4.1",
            name="Cydonia 24B",
            provider="TheDrummer",
            weight=0.02,
            specialization="fine_tuned",
            cost_per_1m_tokens=0.4
        )
        
        models["relace-apply"] = AIModelConfig(
            id="relace/relace-apply-3",
            name="Relace Apply 3",
            provider="Relace",
            weight=0.02,
            specialization="applied_reasoning",
            cost_per_1m_tokens=0.5
        )
        
        models["cogito-70b"] = AIModelConfig(
            id="deepcogito/cogito-v2-preview-llama-70b",
            name="Cogito V2 70B",
            provider="Deep Cogito",
            weight=0.02,
            specialization="efficient_reasoning",
            cost_per_1m_tokens=1.0
        )
        
        models["cogito-109b"] = AIModelConfig(
            id="deepcogito/cogito-v2-preview-llama-109b-moe",
            name="Cogito V2 109B MoE",
            provider="Deep Cogito",
            weight=0.02,
            specialization="mixture_of_experts",
            cost_per_1m_tokens=1.5
        )
        
        models["cogito-deepseek"] = AIModelConfig(
            id="deepcogito/cogito-v2-preview-deepseek-671b",
            name="Cogito V2 DeepSeek 671B",
            provider="Deep Cogito",
            weight=0.02,
            specialization="massive_scale",
            cost_per_1m_tokens=2.0
        )
        
        # Verify total weight = 1.0
        total_weight = sum(m.weight for m in models.values())
        print(f"   Total weight: {total_weight:.2f}")
        
        return models
        
    async def get_trading_decision(self, symbol: str, price: float, 
                                   indicators: Dict, market_context: str) -> Tuple[SignalType, float, List[AIVote]]:
        """
        Get trading decision from all AI models with consensus voting
        
        Returns:
            - Final signal (BUY/SELL/HOLD)
            - Consensus confidence (0-1)
            - List of all AI votes
        """
        print(f"\n{'='*80}")
        print(f"ðŸ¤– ULTIMATE AI HIVE MIND CONSULTATION")
        print(f"{'='*80}")
        print(f"Symbol: {symbol}")
        print(f"Price: ${price:.2f}")
        print(f"Consulting {len(self.models)} AI models...")
        
        # Create prompt for all models
        prompt = self._create_trading_prompt(symbol, price, indicators, market_context)
        
        # Query all models in parallel
        tasks = []
        for model_id, config in self.models.items():
            task = self._query_model(model_id, config, prompt)
            tasks.append(task)
            
        # Gather results
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process votes
        votes = []
        for (model_id, config), result in zip(self.models.items(), results):
            if isinstance(result, Exception):
                print(f"  âš  {config.name}: Error - {result}")
                continue
                
            vote = AIVote(
                model_id=model_id,
                model_name=config.name,
                signal=result['signal'],
                confidence=result['confidence'],
                reasoning=result['reasoning'],
                timestamp=datetime.now(),
                response_time=result.get('response_time', 0)
            )
            votes.append(vote)
            print(f"  âœ“ {config.name}: {vote.signal.value} ({vote.confidence:.0%})")
            
        # Calculate weighted consensus
        final_signal, consensus_confidence = self._calculate_weighted_consensus(votes)
        
        print(f"\n{'='*80}")
        print(f"ðŸ“Š CONSENSUS RESULT")
        print(f"{'='*80}")
        print(f"Signal: {final_signal.value}")
        print(f"Confidence: {consensus_confidence:.2%}")
        print(f"Votes collected: {len(votes)}/{len(self.models)}")
        print(f"{'='*80}\n")
        
        return final_signal, consensus_confidence, votes
        
    def _create_trading_prompt(self, symbol: str, price: float, 
                              indicators: Dict, market_context: str) -> str:
        """Create comprehensive trading prompt"""
        return f"""You are an expert algorithmic trading AI. Analyze this trading opportunity:

**Symbol:** {symbol}
**Current Price:** ${price:.2f}

**Technical Indicators:**
{json.dumps(indicators, indent=2)}

**Market Context:**
{market_context}

**Task:**
Provide a trading decision with the following JSON format:
{{
    "signal": "BUY" | "SELL" | "HOLD" | "STRONG_BUY" | "STRONG_SELL",
    "confidence": 0.0-1.0,
    "reasoning": "Brief 1-2 sentence explanation"
}}

Consider:
1. Technical indicator signals
2. Market conditions
3. Risk/reward ratio
4. Trend strength
5. Volume confirmation

Respond ONLY with valid JSON."""
        
    async def _query_model(self, model_id: str, config: AIModelConfig, prompt: str) -> Dict:
        """Query a single AI model"""
        start_time = datetime.now()
        
        try:
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openrouter_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://trading-system.ai",
                    "X-Title": "Ultimate Trading System"
                },
                json={
                    "model": config.id,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 300,
                    "temperature": 0.3
                },
                timeout=30
            )
            
            response_time = (datetime.now() - start_time).total_seconds()
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                # Parse JSON response
                # Try to extract JSON from markdown code blocks if present
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                    
                data = json.loads(content)
                data['signal'] = SignalType[data['signal']]
                data['response_time'] = response_time
                return data
            else:
                raise Exception(f"API error: {response.status_code} - {response.text}")
                
        except Exception as e:
            # Return neutral vote on error
            return {
                "signal": SignalType.HOLD,
                "confidence": 0.0,
                "reasoning": f"Error: {str(e)}",
                "response_time": (datetime.now() - start_time).total_seconds()
            }
            
    def _calculate_weighted_consensus(self, votes: List[AIVote]) -> Tuple[SignalType, float]:
        """Calculate weighted consensus from all votes"""
        if not votes or len(votes) < self.min_votes_required:
            return SignalType.HOLD, 0.0
            
        # Calculate weighted scores for each signal type
        signal_scores = {
            SignalType.STRONG_BUY: 0.0,
            SignalType.BUY: 0.0,
            SignalType.HOLD: 0.0,
            SignalType.SELL: 0.0,
            SignalType.STRONG_SELL: 0.0
        }
        
        total_weight = 0.0
        
        for vote in votes:
            model_id = vote.model_id
            if model_id not in self.models:
                continue
                
            model_config = self.models[model_id]
            weight = model_config.weight
            
            # Weight by both model weight and confidence
            weighted_score = weight * vote.confidence
            signal_scores[vote.signal] += weighted_score
            total_weight += weight
            
        # Normalize scores
        if total_weight > 0:
            for signal in signal_scores:
                signal_scores[signal] /= total_weight
                
        # Get signal with highest score
        best_signal = max(signal_scores.items(), key=lambda x: x[1])
        consensus_signal = best_signal[0]
        consensus_confidence = best_signal[1]
        
        # Simplify STRONG signals to regular if confidence is low
        if consensus_confidence < 0.8:
            if consensus_signal == SignalType.STRONG_BUY:
                consensus_signal = SignalType.BUY
            elif consensus_signal == SignalType.STRONG_SELL:
                consensus_signal = SignalType.SELL
                
        return consensus_signal, consensus_confidence
        
    def get_model_statistics(self) -> Dict:
        """Get statistics about the AI hive mind"""
        specializations = {}
        for config in self.models.values():
            spec = config.specialization
            if spec not in specializations:
                specializations[spec] = []
            specializations[spec].append(config.name)
            
        return {
            "total_models": len(self.models),
            "total_weight": sum(m.weight for m in self.models.values()),
            "specializations": specializations,
            "providers": list(set(m.provider for m in self.models.values())),
            "avg_cost_per_1m": sum(m.cost_per_1m_tokens for m in self.models.values()) / len(self.models)
        }


async def test_hive_mind():
    """Test the ultimate AI hive mind"""
    print("\n" + "="*80)
    print("TESTING ULTIMATE AI HIVE MIND - 30 MODELS")
    print("="*80 + "\n")
    
    # Initialize hive mind
    hive_mind = UltimateAIHiveMind()
    
    # Print statistics
    stats = hive_mind.get_model_statistics()
    print(f"\nðŸ“Š Hive Mind Statistics:")
    print(f"  Total Models: {stats['total_models']}")
    print(f"  Total Weight: {stats['total_weight']:.2f}")
    print(f"  Providers: {len(stats['providers'])}")
    print(f"  Specializations: {len(stats['specializations'])}")
    print(f"  Avg Cost/1M tokens: ${stats['avg_cost_per_1m']:.2f}")
    
    print(f"\nðŸŽ¯ Specializations:")
    for spec, models in stats['specializations'].items():
        print(f"  {spec}: {len(models)} models")
        
    # Test with sample data
    print(f"\nðŸ§ª Testing with sample BTC/USDT analysis...")
    
    sample_indicators = {
        "rsi_14": 65.5,
        "macd": 0.0025,
        "macd_signal": 0.0020,
        "bollinger_upper": 45000,
        "bollinger_middle": 43500,
        "bollinger_lower": 42000,
        "volume_ratio": 1.35
    }
    
    sample_context = """
    Market is showing bullish momentum with increasing volume.
    RSI indicates strength but not overbought.
    MACD showing bullish crossover.
    Price testing upper Bollinger band.
    """
    
    signal, confidence, votes = await hive_mind.get_trading_decision(
        symbol="BTC/USDT",
        price=43800.00,
        indicators=sample_indicators,
        market_context=sample_context
    )
    
    print(f"\nâœ… Test Complete!")
    print(f"   Final Signal: {signal.value}")
    print(f"   Consensus Confidence: {confidence:.2%}")
    print(f"   Votes Received: {len(votes)}")


if __name__ == "__main__":
    asyncio.run(test_hive_mind())

